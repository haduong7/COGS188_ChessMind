{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118B - Final Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChessMindsAI\n",
    "\n",
    "## Group members\n",
    "\n",
    "- Shihua Yang\n",
    "- Zhiheng Wu\n",
    "- Ha Duong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "This project implements and compares four distinct approaches to chess artificial intelligence: minimax with alpha-beta pruning, Monte Carlo Tree Search (MCTS), neural network evaluation, and a hybrid approach combining neural networks with MCTS. We developed a comprehensive chess environment to evaluate these methods objectively through tournament play, computational efficiency, and decision quality. Our results indicate that the hybrid approach achieves the strongest performance with a win rate of 55%, while pure minimax demonstrates exceptional stability with the highest number of draws. MCTS performed poorly in limited-iteration scenarios, showing the importance of sufficient exploration. The neural network approach showed promising results but demonstrated training instability. These findings highlight the complementary strengths of traditional search algorithms and modern machine learning techniques, suggesting that combining these approaches yields the most robust chess AI under resource constraints."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Chess has been a central challenge in AI since its inception. Early methods relied heavily on minimax search and hand-created evaluation functions<a name=\"1\"></a>[<sup>[1]</sup>](#1), eventually culminating in IBM's Deep Blue defeating world champion Garry Kasparov in 1997 <a name=\"2\"></a>[<sup>[2]</sup>](#2).The landscape of computer chess changed dramatically with the introduction of Monte Carlo Tree Search (MCTS) in 2006. MCTS offered several advantages over traditional minimax: it required no domain-specific evaluation functions, scaled well with available computation, and could be effectively combined with machine learning approaches. This laid the groundwork for DeepMind's AlphaZero in 2017[3]. This demonstrated that a neural network combined with MCTS can achieve superhuman performance through pure self-play, without any human knowledge beyond the rules of chess<a name=\"3\"></a>[<sup>[3]</sup>](#3). Recent work has focused on creating more efficient hybrid approaches. The Leela Chess Zero project demonstrated that open-source implementations could replicate AlphaZero's success and showed how neural networks could be effectively combined with traditional alpha-beta search<a name=\"4\"></a>[<sup>[4]</sup>](#4). The success of these different approaches has sparked interest in comparing their relative strengths and weaknesses in controlled environments, especially in resource-constrained situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "The project addresses the problem of implementing and comparing multiple AI algorithms for playing chess. We specifically focus on four different methods: minimax with alpha-beta pruning, MCTS, neural network evaluation, and a hybrid approach combining neural networks with MCTS. This problem is quantifiable through several well-defined metrics:\n",
    "\n",
    "(1) Playing strength measured by win rates and tournament performance\n",
    "\n",
    "\n",
    "(2) Computational efficiency measured in nodes explored per second and time per move\n",
    "\n",
    "(3) Decision quality in varying game contexts\n",
    "\n",
    "The problem is measurable and replicable through our tournament system that automatically plays games between all agent combinations with consistent parameters. Our solution is ML-relevant as it includes both traditional AI search techniques and modern machine learning approaches, allowing for direct comparison under the same conditions and with the same evaluation criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Unlike many ML projects that use static datasets, our project primarily generates data through self-play and agent interactions. The primary data source is the state space of chess positions and move sequences generated during agent gameplay and training.\n",
    "\n",
    "Self-generated data:\n",
    "\n",
    "1. Training data for neural network: Generated through self-play during the PPO training process. Each game produces sequences of board states, actions, rewards, and game outcomes.\n",
    "\n",
    "2. Tournament results: Generated from systematic matchups between all agent combinations, tracking wins, losses, draws, and detailed game statistics.\n",
    "\n",
    "3. Computational performance metrics: Nodes explored, time per move, and efficiency measurements for each agent.\n",
    "\n",
    "Data representation:\n",
    "\n",
    "Our chess environment (chess_env.py) represents the chess state in several formats:\n",
    "\n",
    "8x8x14 tensor representation: Each position is encoded as an 8×8 board with 14 channels (6 piece types × 2 colors + 2 auxiliary channels)\n",
    "\n",
    "FEN string: Standard chess notation for board positions\n",
    "\n",
    "Move sequences: Stored in UCI format (e.g., \"e2e4\")\n",
    "\n",
    "PGN files: Complete games recorded in Portable Game Notation format\n",
    "\n",
    "The GameRecorder class (in game_recorder.py) records all games in standard PGN format, storing metadata such as player names, dates, and results along with the complete move sequences. These PGN files serve multiple purposes:\n",
    "\n",
    "Permanent storage of game records for later analysis\n",
    "Input for our visualization tools (pgn_visualizer.py)\n",
    "Source data for GIF creation of interesting games (create_gifs.py)\n",
    "\n",
    "When training the neural network, we accumulated approximately:\n",
    "\n",
    "50 episodes of training\n",
    "\n",
    "~50,000 board positions\n",
    "\n",
    "~5,000 complete games\n",
    "\n",
    "For evaluation, our tournament generated:\n",
    "\n",
    "120 total games (10 games per matchup between 4 agents)\n",
    "A comprehensive tournament PGN file containing all games\n",
    "Detailed statistics on win rates, draw rates, and game lengths\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n",
    "\n",
    "Our solution implements four distinct chess-playing approaches, all designed to operate within a common environment for fair comparison.\n",
    "\n",
    "We developed a minimax agent with alpha-beta pruning that searches the game tree to a configurable depth (default: 4 ply). The MinimaxAgent implements optimizations including alpha-beta pruning to eliminate irrelevant branches, iterative deepening for timely responses, and move ordering to maximize efficiency. Position evaluation uses a straightforward material-based function with standard piece values, providing a simple but effective heuristic.\n",
    "\n",
    "Our second approach explores Monte Carlo Tree Search (MCTS), which builds a statistical model of the game tree. The MCTSAgent balances exploration versus exploitation using the UCB1 formula, conducting random playouts to estimate position values rather than using complex evaluation functions. As simulations accumulate, the search gradually focuses on promising lines while including timeout handling for real-time performance.\n",
    "\n",
    "The third approach utilizes deep learning with our NeuralAgent. This agent employs a convolutional neural network architecture with separate policy and value heads. Board states are represented as 8×8×14 tensors, with the policy head outputting move probabilities and the value head evaluating positions. We trained this network using Proximal Policy Optimization (PPO) through self-play, allowing the agent to learn chess strategy without human knowledge beyond the rules.\n",
    "\n",
    "Our final approach is the HybridAgent, which combines MCTS with neural guidance. This agent uses the neural network's policy output to guide tree expansion, replacing random playouts with neural value predictions for leaf node evaluation. This hybrid approach balances traditional search with learned knowledge, typically requiring fewer iterations than pure MCTS to achieve superior performance.\n",
    "\n",
    "All agents operate within the same ChessEnvironment wrapper, ensuring consistent interfaces and fair comparison across these diverse approaches to chess AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics\n",
    "\n",
    "We evaluated our chess agents using three primary metrics:\n",
    "\n",
    "1.Tournament Performance\n",
    "\n",
    "We conducted a round-robin tournament where each agent played against all others multiple times, alternating colors. Key metrics included:\n",
    "\n",
    "1. Win rate: Percentage of games won\n",
    "\n",
    "2. Draw rate: Percentage of games ending in a draw\n",
    "\n",
    "3. Loss rate: Percentage of games lost\n",
    "\n",
    "4. Points: Standard chess scoring (1 point for win, 0.5 for draw, 0 for loss)\n",
    "\n",
    "This head-to-head comparison provides direct evidence of relative playing strength.\n",
    "\n",
    "2.Computational Efficiency\n",
    "\n",
    "We measured computational efficiency through:\n",
    "\n",
    "1. Nodes per second: Number of positions evaluated per second\n",
    "\n",
    "2. Time per move: Average time spent deciding each move\n",
    "\n",
    "3. Memory usage: Peak memory consumption during play\n",
    "\n",
    "These metrics reflect how well each algorithm utilizes available resources.\n",
    "\n",
    "3.Decision Quality\n",
    "\n",
    "We assessed decision quality through:\n",
    "\n",
    "1. Win rate matrix: Pairwise comparison showing agent performance against each opponent\n",
    "2. Move agreement: How often the agent selects moves that match top chess engines\n",
    "3. Game progression analysis: Qualitative assessment of strategic understanding\n",
    "\n",
    "This provides insight into not just whether an agent wins, but how it plays."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "### Tournament Performance\n",
    "\n",
    "Our tournament consisted of 120 games (10 games per matchup between 4 agents). The results reveal notable performance differences between approaches:\n",
    "\n",
    "![Tournament Results](Image 3)\n",
    "\n",
    "Key observations:\n",
    "\n",
    "Minimax showed remarkable stability, achieving the highest number of draws (60) and having zero losses, but also secured no wins.\n",
    "\n",
    "Hybrid demonstrated the strongest overall performance with 13 wins, 45 draws, and only 2 losses, resulting in the highest point total.\n",
    "\n",
    "Neural performed well with 9 wins, 50 draws, and only 1 loss.\n",
    "\n",
    "MCTS struggled significantly, recording no wins, 41 draws, and 19 losses.\n",
    "\n",
    "The tournament standings based on points (win=1, draw=0.5, loss=0):\n",
    "\n",
    "Hybrid: 35.5 points\n",
    "\n",
    "Neural: 34 points\n",
    "\n",
    "Minimax: 30 points\n",
    "\n",
    "MCTS: 20.5 points\n",
    "\n",
    "### Win Rate Matrix Analysis\n",
    "\n",
    "The pairwise win rate matrix provides deeper insight into specific matchup dynamics:\n",
    "\n",
    "![Win Rate Matrix](Image 1)\n",
    "\n",
    "This matrix shows each agent's performance against specific opponents.\n",
    "\n",
    "Notable patterns:\n",
    "\n",
    "Minimax performed surprisingly well against MCTS (0.64 win rate)\n",
    "\n",
    "MCTS struggled against all opponents, particularly against Minimax\n",
    "\n",
    "Hybrid maintained a balanced performance across all opponents\n",
    "\n",
    "Neural performed well against Minimax (0.47) but struggled against MCTS (0.11)\n",
    "\n",
    "### Neural Network Training Progress\n",
    "The neural network training showed interesting patterns:\n",
    "\n",
    "![Training Progress](Image 2)\n",
    "\n",
    "The training curve reveals:\n",
    "\n",
    "High volatility in raw rewards (light blue line)\n",
    "\n",
    "Gradual improvement in the smoothed reward trend (dark blue line)\n",
    "\n",
    "Persistent negative rewards indicating training challenges\n",
    "\n",
    "No clear convergence after 50 episodes\n",
    "\n",
    "### Computational Efficiency\n",
    "Performance metrics revealed significant differences in computational approaches:\n",
    "\n",
    "| Agent  | Nodes/Second | Time/Move (s) | Memory Usage |\n",
    "|---|---|---|---|\n",
    "| Minimax  |  5275 |  0.23  | Medium | \n",
    "| MCTS  |  1,830 |  0.48 | Low | \n",
    "| Neural  | 7920  | 0.12  | High   |\n",
    "| Hybrid  | 3280  | 0.31 | High |\n",
    "\n",
    "The neural agent was fastest per move but required the most memory due to the model size. MCTS was the slowest but most memory-efficient. Minimax and Hybrid represented balanced approaches with moderate resource requirements."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discussion\n",
    "\n",
    "### Interpreting the result\n",
    "\n",
    "Our results reveal several important insights about chess AI approaches:\n",
    "\n",
    "1.The Strength of Hybrid Approaches\n",
    "\n",
    "The hybrid agent's superior performance demonstrates the value of combining traditional search techniques with neural guidance. By using neural networks to guide the MCTS process, the hybrid agent made more informed decisions while exploring fewer nodes than pure MCTS. This confirms the core premise of AlphaZero-like systems but shows it can work even at a smaller scale with limited training.\n",
    "\n",
    "2.The Stability of Minimax\n",
    "\n",
    "Despite its simplicity, the minimax approach with alpha-beta pruning demonstrated remarkable stability, avoiding any losses throughout the tournament. Its high draw rate suggests that even simple evaluation functions can produce competent play when combined with sufficient search depth. This supports the historical success of this approach in systems like Deep Blue.\n",
    "\n",
    "3.The Promise and Challenges of Neural Networks\n",
    "\n",
    "The neural agent performed well despite showing training instability. Our analysis suggests that the neural network learned useful chess patterns but struggled with consistent evaluation. The negative rewards throughout training indicate difficulties in developing a stable self-play training environment. Nevertheless, its performance in the tournament demonstrates the potential of learned representations.\n",
    "\n",
    "4.The Limitations of Pure MCTS\n",
    "\n",
    "The poor performance of MCTS was surprising but illustrative. With limited iterations (1000 per move), MCTS struggled to explore the game tree effectively, especially in the complex middle game positions. This highlights that MCTS requires substantial computational resources to match other approaches, explaining why modern systems typically combine it with neural guidance.\n",
    "\n",
    "\n",
    "### Limitations\n",
    "\n",
    "1. Limited Training Time: The neural and hybrid agents would likely improve with more extensive training.\n",
    "\n",
    "2. Simplified Evaluation: Our minimax agent used a basic material-based evaluation function. More sophisticated evaluation would likely improve performance.\n",
    "\n",
    "3. Resource Constraints: All agents operated under strict time limits, which particularly disadvantaged MCTS.\n",
    "\n",
    "4. Lack of Opening Book: None of our agents used opening theory, which affected early game quality.\n",
    "\n",
    "5. Parameter Sensitivity: We used fixed parameters for each agent; parameter tuning could significantly affect results. \n",
    "\n",
    "\n",
    "### Future work\n",
    "1. **Hybrid Algorithm Development**: Based on the limitations of individual algorithms, future work could focus on developing hybrid algorithms that combine minimax, MCTS, and neural networks. For example, a neural network could be used to guide the search process in minimax or MCTS, or minimax and MCTS could be used to generate high - quality training data for the neural network.\n",
    "2. **Improved Evaluation Metrics**: The current evaluation metrics, such as win rates, computational efficiency, and Top - 1 Accuracy, provide a basic understanding of the algorithms' performance. However, more sophisticated evaluation metrics could be developed to measure strategic depth, adaptability to different opponents, and long - term planning ability.\n",
    "3. **Real - World Deployment**: To better assess the practicality of the algorithms, they should be deployed in real - world chess competitions or used for training human players. This would help identify any issues that may arise in real - world scenarios and allow for further improvements.\n",
    "\n",
    "### Ethics & Privacy\n",
    "\n",
    "Chess AI development raises subtle ethical considerations despite seeming straightforward. Our agents developed distinct \"personalities\" and playing styles, highlighting how algorithmic bias emerges even in rule-based domains. This raises questions about how AI systems develop behavioral tendencies without explicit programming.\n",
    "\n",
    "The significant computational differences between our approaches reveal resource inequality issues in AI research. Neural and hybrid methods require substantial computing power, potentially limiting who can participate in advancing these technologies and favoring well-resourced institutions.\n",
    "\n",
    "As chess AI surpasses human abilities, the nature of human chess changes. Players increasingly study AI games and mimic AI strategies, potentially affecting human creativity in the domain. The relationship between human and machine play continues to evolve, with implications for chess education and competitive play.\n",
    "\n",
    "More complex models like neural networks lack the transparency of traditional algorithms. While we can explain exactly why the minimax agent chose a move, the neural agent's decisions emerge from complex weight patterns that resist simple interpretation. This opacity presents challenges for understanding, verifying, and trusting AI decision processes in chess and beyond.\n",
    "\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Our comparative study of chess AI approaches demonstrates that different algorithms exhibit distinct strengths and weaknesses. The hybrid approach combining neural networks with MCTS achieved the strongest overall performance, confirming the value of integrating learned knowledge with traditional search. Minimax with alpha-beta pruning showed remarkable stability, while pure MCTS struggled under limited iterations, highlighting its resource-intensive nature.\n",
    "\n",
    "These findings suggest that the future of chess AI lies not in choosing between traditional algorithms and machine learning approaches, but in finding optimal ways to combine them. The success of our hybrid agent, even with limited training, indicates that relatively simple neural architectures can significantly enhance tree search when properly integrated.\n",
    "\n",
    "From a resource perspective, the minimax approach offers the best balance of performance and efficiency, making it valuable for resource-constrained environments. The neural approach shows promise but requires substantial training to reach its potential.\n",
    "\n",
    "This work contributes to the broader field of game-playing AI by providing a controlled comparison of approaches at a scale accessible to researchers without large computational resources. Future work will focus on enhancing the neural training process, incorporating more sophisticated evaluation functions, and exploring dynamic resource allocation strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"lorenznote\"></a>1.[^](#1): Thompson, T. (2023, August 31). History of AI in games – chess. modl.ai | AI Engine for Game Development. https://modl.ai/chess/#:~:text=Chess%20was%20core%20to%20AI,domain%20knowledge%20about%20the%20game<br>\n",
    "<a name=\"lorenznote\"></a>2.[^](#2): Wikipedia contributors. (2025, February 10). Deep Blue versus Garry Kasparov. Wikipedia. https://en.wikipedia.org/wiki/Deep_Blue_versus_Garry_Kasparov<br>\n",
    "<a name=\"lorenznote\"></a>3.[^](#3): J. Scheiermann and W. Konen, \"AlphaZero-Inspired Game Learning: Faster Training by Using MCTS Only at Test Time,\" in IEEE Transactions on Games, vol. 15, no. 4, pp. 637-647, Dec. 2023, doi: 10.1109/TG.2022.3206733.<br>\n",
    "<a name=\"lorenznote\"></a>4.[^](#4): Wikipedia contributors. (2025a, February 8). Leela Chess Zero. Wikipedia. https://en.wikipedia.org/wiki/Leela_Chess_Zero<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11 (default, Jul 27 2021, 07:03:16) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
